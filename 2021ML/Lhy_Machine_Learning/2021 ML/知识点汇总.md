《机器学习基本概念简介上》
Error Surface: 使用不用的w,b去画不出来的loss的等高线图
Gradient Descent: 
    - w,b= arg minLoss 找w,b这样的参数来使loss最小
    - 首先来选择一个w0,可以是任意的值
    - w0在loss曲线上的微分，σL/σw|w = w0
    - 有时候w0的下降的斜率很大w就大一些，反之小
    - 当然还需要设置learning rate，这个是自己设置的，相当于放大了微分的操作，就是微信的系数
    - 在机器学习的训练中需要自己设置的值就叫做hyperparameters

在Gradient Descent的时候，当loss为0时候不一定是global minima,也可能是local minima

训练的定义和过程：
    -你不知道一个函数function with unknown
    -定义loss
    -找一些参数，让loss最小


《机器学习基本概念简介下》
linear models会带来model bias:
    -使用直线去拟合的话，模型太简单了
    
sigmoid models会更好的拟合函数

为什么需要使用sigmoid function?
    -训练第一步去宏观的描述一个函数，参数都是未知的，使用多个sigmoid function叠加，会生成一个任意的曲线。
    -如何去拟合一个函数？ 
        y= b + i个CiSigmoid(bi+wi*x1) 见图片

Attention:
    -不同的sigmoid函数只能去拟合一段任意的函数，如果函数的定义域很广，就需要更多的sigmoid函数来进行拟合。所以在拟合函数中有i,j

seita是怎么来的？
    -其实就是所有拟合函数的参数
    -

many layers means deep learning

overfitting


# self attention
- 多个向量的输入就需要
- 多个向量输入，输出多个向量
- 整个的过程是a通过w 去转置为q k v，然后 q 再和k的转置为A'  v再和A'生成O 
- self attention是一个简单的cnn，
- RNN只考虑单个方向的， self attention考虑之前的。


# soft-max
1. 把分数转换成概率
2. 

# multi-head self-attentiond
1. 就是有多个q ，多个k v 
        
# positional Encoding
1. 给a加入e的位置的信息
2. 

```````````````````````````````
# Transformer
1. seq2seq的项目都是transf
2. 的d

# seq2seq for multi-label classifcation
an object can belong to multiple classes.

# residual connect是把input和output直接加起来。
残差 = 观测到的真实值 (Observed Value) - 模型预测的值 (Predicted Value)

# Layer Norm
	                 Batch Norm (BN)	                    Layer Norm (LN)
核心思想	依赖批次：需要看整个批次的数据才能工作。	独立自主：只看自己这一个样本的数据。
可以理解为	适用于**“能用大 Batch”**的场景。	      适用于**“Batch 很小或变长”**的场景。
典型应用	CNN (图像)                                 Transformer / RNN (文本序列)

# 